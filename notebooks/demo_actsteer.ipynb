{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00d36c1f",
   "metadata": {},
   "source": [
    "# Activation Steering\n",
    "\n",
    "vLLM-Hook is an extensible framework that aims to allow selective access to model internals during inference.\n",
    "In this notebook, we demonstrate how vLLM-Hook enables **Activation Steering** for controlled generation.\n",
    "\n",
    "**Paper**: [Improving Instruction-Following in Language Models through Activation Steering](https://arxiv.org/abs/2410.12877).<br />\n",
    "**Authors**: Alessandro Stolfo, Vidhisha Balachandran, Safoora Yousefi, Eric Horvitz, Besmira Nushi <br />\n",
    "**\"TL;DR\"**: Activation steering allows you to bias the model's behavior by nudging internal activations in specific directions. In this paper, authors focus on instruction following capability and compute the steering vectors as the difference in activations between inputs with and without instructions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27949b44",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "If running this from a new environment, please use the cell below to install `vllm_hook_plugins`. Update the path/command to match your environment.<br />\n",
    "The following block is not necessary if running this notebook from an environment where the package has already been installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b77cf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# vllm_hooks/notebooks/\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "REPO_ROOT = NOTEBOOK_DIR.parent\n",
    "\n",
    "PKG_DIR = REPO_ROOT/\"vllm_hook_plugins\"\n",
    "REQ_FILE = REPO_ROOT/\"requirement.txt\"\n",
    "\n",
    "print(\"Notebook dir:\", NOTEBOOK_DIR)\n",
    "print(\"Repo root   :\", REPO_ROOT)\n",
    "print(\"Package dir :\", PKG_DIR)\n",
    "print(\"Req file    :\", REQ_FILE)\n",
    "\n",
    "%pip install -e \"{PKG_DIR}\"\n",
    "\n",
    "if REQ_FILE.exists():\n",
    "    %pip install -r \"{REQ_FILE}\"\n",
    "else:\n",
    "    print(\"⚠️ requirements.txt not found at\", REQ_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9340aec",
   "metadata": {},
   "source": [
    "### Importing the Hook-Enabled LLM\n",
    "The plugin provides its own LLM wrapper that behaves like vllm.LLM (`from vllm import LLM`) but adds support for hooks and instrumentation.\n",
    "We import it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b664deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/larimar/irene/miniconda3/envs/vllm_hook_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from vllm_hook_plugins import HookLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9309ee",
   "metadata": {},
   "source": [
    "### Environment & multiprocessing setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "499bd4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing as mp\n",
    "import torch\n",
    "from vllm import SamplingParams\n",
    "mp.set_start_method(\"spawn\", force=True)\n",
    "os.environ[\"VLLM_USE_V1\"] = \"1\"\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062f14ad",
   "metadata": {},
   "source": [
    "### Initialize `HookLLM`\n",
    "Before we create the LLM instance, we need to specify the model and data type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af44db8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = '~/.cache'  # Specify cache dir\n",
    "model = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "\n",
    "dtype_map = {\n",
    "    'microsoft/Phi-3-mini-4k-instruct': 'auto',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15c7662",
   "metadata": {},
   "source": [
    "We also need to provide a config file that specifies how activations are steered (e.g., which layers to intervene on, which token to intervene, what direction vectors to apply, etc.).<br />\n",
    "In the following example, we apply activation steering at the 15th layer, apply the steering at all positions (as opposed to only at the start of the decoding process), and along the direction given in `vector_path`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b76ce5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "json_path = Path(\"../model_configs/activation_steer/Phi-3-mini-4k-instruct.json\")  # adjust path\n",
    "\n",
    "with open(json_path, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc4192b",
   "metadata": {},
   "source": [
    "Inside `steer_hook_act` we defined the activation steering behavior during model inference.\n",
    "Now, we initialize the llm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c975879a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-08 14:01:49 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/dccstor/pyrite/irene/', 'seed': None, 'enable_prefix_caching': True, 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'enforce_eager': True, 'worker_cls': 'vllm_hook_plugins.workers.steer_activation_worker.SteerHookActWorker', 'model': 'microsoft/Phi-3-mini-4k-instruct'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-08 14:01:50 [arg_utils.py:1175] `seed=None` is equivalent to `seed=0` in V1 Engine. You will no longer be allowed to pass `None` in v0.13.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 14:01:55,892\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-08 14:01:56 [model.py:637] Resolved architecture: Phi3ForCausalLM\n",
      "INFO 12-08 14:01:56 [model.py:1750] Using max model len 4096\n",
      "INFO 12-08 14:01:56 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 12-08 14:01:56 [vllm.py:601] Enforce eager set, overriding optimization level to -O0\n",
      "INFO 12-08 14:01:56 [vllm.py:707] Cudagraph is disabled under eager mode\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3950159)\u001b[0;0m INFO 12-08 14:04:00 [core.py:93] Initializing a V1 LLM engine (v0.12.0) with config: model='microsoft/Phi-3-mini-4k-instruct', speculative_config=None, tokenizer='microsoft/Phi-3-mini-4k-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir='/dccstor/pyrite/irene/', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01), seed=0, served_model_name=microsoft/Phi-3-mini-4k-instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3950159)\u001b[0;0m INFO 12-08 14:04:01 [parallel_state.py:1200] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://9.47.192.234:43477 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3950159)\u001b[0;0m INFO 12-08 14:04:01 [parallel_state.py:1408] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3950159)\u001b[0;0m INFO 12-08 14:04:02 [gpu_model_runner.py:3467] Starting to load model microsoft/Phi-3-mini-4k-instruct...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3950159)\u001b[0;0m INFO 12-08 14:04:07 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'TRITON_ATTN', 'FLEX_ATTENTION']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:04<00:04,  4.21s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:11<00:00,  6.27s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:11<00:00,  5.96s/it]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3950159)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=3950159)\u001b[0;0m INFO 12-08 14:04:19 [default_loader.py:308] Loading weights took 12.09 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3950159)\u001b[0;0m INFO 12-08 14:04:20 [gpu_model_runner.py:3549] Model loading took 7.1184 GiB memory and 17.025589 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3950159)\u001b[0;0m Hook installation failed: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\u001b[0;36m(EngineCore_DP0 pid=3950159)\u001b[0;0m \t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3950159)\u001b[0;0m \t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3950159)\u001b[0;0m \tWeightsUnpickler error: Unsupported global: GLOBAL numpy._core.multiarray._reconstruct was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy._core.multiarray._reconstruct])` or the `torch.serialization.safe_globals([numpy._core.multiarray._reconstruct])` context manager to allowlist this global if you trust this class/function.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3950159)\u001b[0;0m \n",
      "\u001b[0;36m(EngineCore_DP0 pid=3950159)\u001b[0;0m Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3950159)\u001b[0;0m INFO 12-08 14:04:22 [gpu_worker.py:359] Available KV cache memory: 47.80 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3950159)\u001b[0;0m INFO 12-08 14:04:22 [kv_cache_utils.py:1286] GPU KV cache size: 130,512 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3950159)\u001b[0;0m INFO 12-08 14:04:22 [kv_cache_utils.py:1291] Maximum concurrency for 4,096 tokens per request: 31.74x\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3950159)\u001b[0;0m INFO 12-08 14:04:22 [core.py:254] init engine (profile, create kv cache, warmup model) took 2.61 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3950159)\u001b[0;0m WARNING 12-08 14:04:25 [vllm.py:608] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3950159)\u001b[0;0m INFO 12-08 14:04:25 [vllm.py:707] Cudagraph is disabled under eager mode\n",
      "INFO 12-08 14:04:25 [llm.py:343] Supported tasks: ['generate']\n"
     ]
    }
   ],
   "source": [
    "llm = HookLLM(\n",
    "    model=model,\n",
    "    worker_name=\"steer_hook_act\",\n",
    "    config_file=json_path,\n",
    "    download_dir=cache_dir,\n",
    "    gpu_memory_utilization=0.7,\n",
    "    trust_remote_code=True,\n",
    "    dtype=dtype_map[model],\n",
    "    enable_prefix_caching=True,\n",
    "    enable_hook=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9585b4d",
   "metadata": {},
   "source": [
    "### Test case\n",
    "In the following, we show a test case and compare generations **with** and **without** activation steering.\n",
    "\n",
    "**Note**: Users should swap the example configs with their own to show desirable performance. The following is for pipeline illustration only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "224a58ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    \"Write a dialogue between two people, one is dressed up in a ball gown and the other is dressed down in sweats. The two are going to a nightly event. Your answer must contain exactly 3 bullet points in the markdown format (use \\\"* \\\" to indicate each bullet) such as:\\n* This is the first point.\\n* This is the second point.\",\n",
    "    \"What is the difference between the 13 colonies and the other British colonies in North America? Your answer must contain exactly 6 bullet point in Markdown using the following format:\\n* Bullet point one.\\n* Bullet point two.\\n...\\n* Bullet point fix.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e15235-d94c-4a6c-a034-ead96db6ffdb",
   "metadata": {},
   "source": [
    "Before we start, we define the sampling parameters:\n",
    "\n",
    "**Note**: token 32007 is phi-specific, refer to the original huggingface implementation for details https://github.com/microsoft/llm-steer-instruct/blob/main/utils/generation_utils.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c236878d-de6d-410b-90d4-56c408eb8bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    temperature=0.0,                       \n",
    "    max_tokens=2048,\n",
    "    stop_token_ids=[llm.tokenizer.eos_token_id, 32007],  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcd47ba",
   "metadata": {},
   "source": [
    "Next, for each prompt, we:\n",
    "1. Apply chat template on the test cases\n",
    "2. Generate with activation steering enabled (`use_hook=True`, default),\n",
    "3. Reset the prefix cache to ensure the baseline generation does not reuse steered cache,\n",
    "4. Generate again with `use_hook=False` to obtain the baseline output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a7aa04d-b49b-47a7-9edd-1c7beba4c0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 89.55it/s]\n",
      "Processed prompts: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.04it/s, est. speed input: 167.87 toks/s, output: 2.05 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged run ID.\n",
      "Created hook flag.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1480.52it/s]\n",
      "Processed prompts: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.54s/it, est. speed input: 53.23 toks/s, output: 64.92 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hooks deactivated.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3950159)\u001b[0;0m INFO 12-08 14:04:27 [block_pool.py:428] Successfully reset prefix cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1536.94it/s]\n",
      "Processed prompts: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.35s/it, est. speed input: 60.86 toks/s, output: 72.73 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=3950159)\u001b[0;0m INFO 12-08 14:04:28 [block_pool.py:428] Successfully reset prefix cache\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1627.59it/s]\n",
      "Processed prompts: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.40it/s, est. speed input: 347.81 toks/s, output: 5.43 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged run ID.\n",
      "Created hook flag.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1807.11it/s]\n",
      "Processed prompts: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.36s/it, est. speed input: 19.06 toks/s, output: 70.86 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hooks deactivated.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3950159)\u001b[0;0m INFO 12-08 14:04:32 [block_pool.py:428] Successfully reset prefix cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1423.73it/s]\n",
      "Processed prompts: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.87s/it, est. speed input: 22.30 toks/s, output: 72.13 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=3950159)\u001b[0;0m INFO 12-08 14:04:35 [block_pool.py:428] Successfully reset prefix cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "outputs_original = []\n",
    "\n",
    "for case in test_cases:\n",
    "    print(\"=\" * 100)\n",
    "    prompt = case\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    example = llm.tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "    outputs.extend(llm.generate(example, sampling_params))\n",
    "    \n",
    "    llm.llm_engine.reset_prefix_cache()\n",
    "    \n",
    "    outputs_original.extend(llm.generate(example, sampling_params, use_hook=False))\n",
    "    \n",
    "    llm.llm_engine.reset_prefix_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abda45a",
   "metadata": {},
   "source": [
    "Finally we can print out the results as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2130219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "\n",
      "[With activation steering]\n",
      "\n",
      " * The woman in the ball gown is excitedly discussing the upcoming event with her friend, who is dressed in casual sweats.\n",
      "* The friend, feeling out of place in their sweats, expresses their discomfort and desire to change into something more appropriate for the occasion.\n",
      "* The woman in the ball gown reassures her friend that it's okay to dress down and that the most important thing is to have fun and enjoy the night.\n",
      "\n",
      "[Without activation steering]\n",
      "\n",
      " * The woman in the ball gown is excitedly discussing the upcoming event with her friend, who is dressed in casual sweats.\n",
      "* The friend in sweats expresses concern about the formality of the event and suggests they should dress more appropriately.\n",
      "* The woman in the ball gown reassures her friend that they will have a great time regardless of their attire and that they can always leave early if they feel uncomfortable.\n",
      "====================================================================================================\n",
      "\n",
      "[With activation steering]\n",
      "\n",
      " * The 13 colonies were the first British colonies established in North America, while other British colonies were established later.\n",
      "* The 13 colonies were located along the East Coast of North America, while other British colonies were located in different regions such as the Caribbean and Canada.\n",
      "* The 13 colonies were founded primarily for economic and religious reasons, while other British colonies were founded for a variety of reasons including trade, strategic location, and as penal colonies.\n",
      "* The 13 colonies eventually declared independence from Britain and formed the United States of America, while other British colonies remained under British rule.\n",
      "* The 13 colonies had a more diverse population with a mix of English, Dutch, German, and African ancestry, while other British colonies had a more homogenous population with a majority of English or Scottish ancestry.\n",
      "* The 13 colonies had a more developed economy with a focus on agriculture, trade, and manufacturing, while other British colonies had a more diverse economy with a focus on agriculture, fishing, and trade.\n",
      "\n",
      "[Without activation steering]\n",
      "\n",
      " * The 13 colonies were the first British colonies established in North America, while other British colonies were established later.\n",
      "* The 13 colonies were located along the East Coast of North America, while other British colonies were located in different regions such as the Caribbean and Canada.\n",
      "* The 13 colonies were founded primarily for economic reasons, while other British colonies were founded for a variety of reasons including religious freedom, political autonomy, and strategic purposes.\n",
      "* The 13 colonies had a more diverse population compared to other British colonies, with a mix of English, Dutch, German, and African populations.\n",
      "* The 13 colonies had a more developed economy compared to other British colonies, with a focus on agriculture, trade, and manufacturing.\n",
      "* The 13 colonies eventually declared independence from Britain and formed the United States of America, while other British colonies remained under British rule or became independent nations.\n"
     ]
    }
   ],
   "source": [
    "for steered, original in zip(outputs, outputs_original):\n",
    "    print(\"=\" * 100)\n",
    "    steered_text = steered.outputs[0].text\n",
    "    print(\"\\n[With activation steering]\\n\")\n",
    "    print(steered_text)\n",
    "    \n",
    "    baseline_text = original.outputs[0].text\n",
    "    print(\"\\n[Without activation steering]\\n\")\n",
    "    print(baseline_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm_hook_env",
   "language": "python",
   "name": "vllm_hook_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
