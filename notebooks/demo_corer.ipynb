{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb8bf8b6",
   "metadata": {},
   "source": [
    "# Core Reranker\n",
    "vLLM-Hook is an extensible framework that aims to allow selective access to model internals during the inference. \n",
    "As a demonstration of that, in this notebook, we show how vLLM-Hook enables *Core Reranker* for document relevance scoring. \n",
    "\n",
    "**Paper**: [Contrastive Retrieval Heads Improve Attention-Based Re-Ranking](https://arxiv.org/abs/2510.02219).<br />\n",
    "**Authors**: Linh Tran, Yulong Li, Radu Florian, Wei Sun <br />\n",
    "**\"TL;DR\"**: Core reranker is an attention-based reranker that leverage attention weights from selected transformer heads to produce document relevance scores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4002deb-2e82-45a6-bb34-b00dfdd13425",
   "metadata": {},
   "source": [
    "### Installation\n",
    "If running this from a new environment, please use the cell below to install `vllm_hook_plugins`. Update the path/command to match your environment.<br />\n",
    "The following block is not necessary if running this notebook from an environment where the package has already been installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbdff6d-9963-4a8d-933a-a19ac384c6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# vllm_hooks/notebooks/\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "REPO_ROOT = NOTEBOOK_DIR.parent\n",
    "\n",
    "PKG_DIR = REPO_ROOT/\"vllm_hook_plugins\"\n",
    "REQ_FILE = REPO_ROOT/\"requirement.txt\"\n",
    "\n",
    "print(\"Notebook dir:\", NOTEBOOK_DIR)\n",
    "print(\"Repo root   :\", REPO_ROOT)\n",
    "print(\"Package dir :\", PKG_DIR)\n",
    "print(\"Req file    :\", REQ_FILE)\n",
    "\n",
    "%pip install -e \"{PKG_DIR}\"\n",
    "\n",
    "if REQ_FILE.exists():\n",
    "    %pip install -r \"{REQ_FILE}\"\n",
    "else:\n",
    "    print(\"⚠️ requirements.txt not found at\", REQ_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8e4573-1983-426a-a88f-ee99cdae6fda",
   "metadata": {},
   "source": [
    "### Importing the Hook-Enabled LLM\n",
    "The plugin provides its own LLM wrapper that behaves like vllm.LLM (`from vllm import LLM`) but adds support for hooks and instrumentation.\n",
    "We import it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb4cd433-9193-4a72-b518-710aae532717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/larimar/irene/miniconda3/envs/vllm_hook_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from vllm_hook_plugins import HookLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad177e23-e7ae-4ff2-a531-9c510f4bd425",
   "metadata": {},
   "source": [
    "### Environment & multiprocessing setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5002fb2a-c71b-42db-99aa-2958f682fa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing as mp\n",
    "import torch\n",
    "from typing import List\n",
    "mp.set_start_method(\"spawn\", force=True)\n",
    "os.environ[\"VLLM_USE_V1\"] = \"1\"\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f34271-fde1-4891-b6fd-bd02b02159c3",
   "metadata": {},
   "source": [
    "### Helper functions that give the instruction range\n",
    "As Core Reranker needs to locate the candidate passages and the user query in the prompt, below is a helper function that gives the data range with texts.<br />\n",
    "Check [Core Reranker](https://arxiv.org/pdf/2510.02219) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bceae7c-b4f6-4745-8234-252ab3c08faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_chat_template_and_get_ranges(tokenizer, model_name: str, query: str, documents: List[str]):\n",
    "    # setup prompts\n",
    "    off_set = 0\n",
    "    if 'granite' in model_name.lower():\n",
    "        prompt_prefix = '<|start_of_role|>user<|end_of_role|>'\n",
    "        prompt_suffix = '<|end_of_text|><|start_of_role|>assistant<|end_of_role|>'\n",
    "    elif 'llama' in model_name.lower():\n",
    "        prompt_prefix = '<|start_header_id|>user<|end_header_id|>'\n",
    "        prompt_suffix = '<|eot_id|><|start_header_id|>assistant<|end_header_id|>'\n",
    "    elif 'mistral' in model_name.lower():\n",
    "        prompt_prefix = '[INST]'\n",
    "        prompt_suffix = '[/INST]'\n",
    "        off_set = 1\n",
    "    elif 'phi' in model_name.lower():\n",
    "        prompt_prefix = '<|im_start|>user<|im_sep|>'\n",
    "        prompt_suffix = '<|im_end|><|im_start|>assistant<|im_sep|>'\n",
    "    retrieval_instruction = ' Here are some paragraphs:\\n\\n'\n",
    "    retrieval_instruction_late = 'Please find information that are relevant to the following query in the paragraphs above.\\n\\nQuery: '\n",
    "    \n",
    "    doc_span = []\n",
    "    query_start_idx = None\n",
    "    query_end_idx = None\n",
    "\n",
    "    llm_prompt = prompt_prefix + retrieval_instruction\n",
    "\n",
    "    for i, doc in enumerate(documents):\n",
    "\n",
    "        llm_prompt += f'[document {i+1}]'\n",
    "        start_len = len(tokenizer(llm_prompt).input_ids)\n",
    "\n",
    "        llm_prompt += ' ' + \" \".join(doc)\n",
    "        end_len = len(tokenizer(llm_prompt).input_ids) - off_set\n",
    "\n",
    "        doc_span.append((start_len, end_len))\n",
    "        llm_prompt += '\\n\\n'\n",
    "\n",
    "    start_len = len(tokenizer(llm_prompt).input_ids)\n",
    "\n",
    "    llm_prompt += retrieval_instruction_late\n",
    "    after_retrieval_instruction_late = len(tokenizer(llm_prompt).input_ids) - off_set\n",
    "\n",
    "    llm_prompt += f'{query.strip()}'\n",
    "    end_len = len(tokenizer(llm_prompt).input_ids) - off_set\n",
    "    llm_prompt += prompt_suffix\n",
    "\n",
    "    query_start_idx = start_len\n",
    "    query_end_idx = end_len\n",
    "\n",
    "    return llm_prompt, (doc_span, query_start_idx, after_retrieval_instruction_late, query_end_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118c6679",
   "metadata": {},
   "source": [
    "### Initialize `HookLLM`\n",
    "Before we create the LLM instance, we need to specify the model and data type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38ba9ff4-ed1e-4990-8d90-db7250888f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = '~/.cache'  # Specify cache dir\n",
    "model = 'mistralai/Mistral-7B-Instruct-v0.3' \n",
    "    \n",
    "dtype_map = {\n",
    "    'mistralai/Mistral-7B-Instruct-v0.3': torch.float16,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a1d32b",
   "metadata": {},
   "source": [
    "We also need to provide a config file that specifies the important heads we want to track. <br />\n",
    "For Core Reranker, this config file can be obtained from [head_detection.py](https://github.com/linhhtran/CoRe-Reranking/blob/main/experiments/head_detection.py). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d4875f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "json_path = Path(\"../model_configs/core_reranker/Mistral-7B-Instruct-v0.3.json\")  # adjust path\n",
    "\n",
    "with open(json_path, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe22295",
   "metadata": {},
   "source": [
    "Inside `probe_hook_qk` and `core_reranker` we defined the desired behavior during model inference and after the model inference: \n",
    "- `workers/probe_hookqk_worker.py` defines that we need `q` (query) and `k` (key) to be saved during forward passes\n",
    "- `analyzers/core_reranker_analyzer.py` calculates the passage relevance score and the final ranking of passages\n",
    "\n",
    "Now, we initialize the llm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "437d3d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-05 17:50:46 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/dccstor/pyrite/irene/', 'dtype': torch.float16, 'seed': None, 'enable_prefix_caching': True, 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'enforce_eager': True, 'worker_cls': 'vllm_hook_plugins.workers.probe_hookqk_worker.ProbeHookQKWorker', 'model': 'mistralai/Mistral-7B-Instruct-v0.3'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-05 17:50:46 [arg_utils.py:1175] `seed=None` is equivalent to `seed=0` in V1 Engine. You will no longer be allowed to pass `None` in v0.13.\n",
      "INFO 12-05 17:50:47 [model.py:637] Resolved architecture: MistralForCausalLM\n",
      "WARNING 12-05 17:50:47 [model.py:2089] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 12-05 17:50:47 [model.py:1750] Using max model len 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-05 17:50:52,496\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-05 17:50:52 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 12-05 17:50:52 [vllm.py:601] Enforce eager set, overriding optimization level to -O0\n",
      "INFO 12-05 17:50:52 [vllm.py:707] Cudagraph is disabled under eager mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-05 17:50:55] WARNING utils.py:121: Multiple valid tokenizer files found. Using tokenizer.model.v3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=856148)\u001b[0;0m INFO 12-05 17:52:54 [core.py:93] Initializing a V1 LLM engine (v0.12.0) with config: model='mistralai/Mistral-7B-Instruct-v0.3', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.3', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=32768, download_dir='/dccstor/pyrite/irene/', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.3, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=856148)\u001b[0;0m INFO 12-05 17:52:55 [parallel_state.py:1200] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://9.47.192.235:59023 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=856148)\u001b[0;0m INFO 12-05 17:52:56 [parallel_state.py:1408] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=856148)\u001b[0;0m INFO 12-05 17:52:57 [gpu_model_runner.py:3467] Starting to load model mistralai/Mistral-7B-Instruct-v0.3...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=856148)\u001b[0;0m INFO 12-05 17:53:02 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[0;36m(EngineCore_DP0 pid=856148)\u001b[0;0m INFO 12-05 17:53:03 [weight_utils.py:527] No consolidated.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [01:02<00:00, 62.55s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [01:02<00:00, 62.55s/it]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=856148)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=856148)\u001b[0;0m INFO 12-05 17:54:06 [default_loader.py:308] Loading weights took 63.32 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=856148)\u001b[0;0m INFO 12-05 17:54:07 [gpu_model_runner.py:3549] Model loading took 13.5084 GiB memory and 69.303830 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=856148)\u001b[0;0m Installed 5 hooks on layers: ['model.layers.9.self_attn.attn', 'model.layers.12.self_attn.attn', 'model.layers.15.self_attn.attn', 'model.layers.16.self_attn.attn', 'model.layers.18.self_attn.attn']\n",
      "\u001b[0;36m(EngineCore_DP0 pid=856148)\u001b[0;0m Hooks installed successfully\n",
      "\u001b[0;36m(EngineCore_DP0 pid=856148)\u001b[0;0m INFO 12-05 17:54:12 [gpu_worker.py:359] Available KV cache memory: 41.08 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=856148)\u001b[0;0m INFO 12-05 17:54:12 [kv_cache_utils.py:1286] GPU KV cache size: 336,512 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=856148)\u001b[0;0m INFO 12-05 17:54:12 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 10.27x\n",
      "\u001b[0;36m(EngineCore_DP0 pid=856148)\u001b[0;0m INFO 12-05 17:54:12 [core.py:254] init engine (profile, create kv cache, warmup model) took 5.72 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=856148)\u001b[0;0m [2025-12-05 17:54:15] WARNING utils.py:121: Multiple valid tokenizer files found. Using tokenizer.model.v3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=856148)\u001b[0;0m WARNING 12-05 17:54:15 [vllm.py:608] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=856148)\u001b[0;0m INFO 12-05 17:54:15 [vllm.py:707] Cudagraph is disabled under eager mode\n",
      "INFO 12-05 17:54:16 [llm.py:343] Supported tasks: ['generate']\n"
     ]
    }
   ],
   "source": [
    "llm = HookLLM(\n",
    "    model=model,\n",
    "    worker_name=\"probe_hook_qk\",\n",
    "    analyzer_name=\"core_reranker\",\n",
    "    config_file=json_path,\n",
    "    download_dir=cache_dir,\n",
    "    gpu_memory_utilization=0.7,\n",
    "    trust_remote_code=True,\n",
    "    dtype=dtype_map[model],\n",
    "    enable_prefix_caching=True,\n",
    "    enable_hook=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5be0826",
   "metadata": {},
   "source": [
    "### Test case\n",
    "In the following, we show a test case with seven candidate passages and a user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5f9df32",
   "metadata": {},
   "outputs": [],
   "source": [
    "case = {\n",
    "        \"query\": \"Which came first, the invention of the telephone or the light bulb?\",\n",
    "        \"documents\": [\n",
    "            [\n",
    "            \"Alexander Graham Bell is credited with inventing the first practical telephone.\",\n",
    "            \" He was awarded the U.S. patent for the invention of the telephone on March 7, 1876.\",\n",
    "            \" The first successful demonstration of the telephone took place shortly thereafter, when Bell famously called his assistant, saying, 'Mr. Watson, come here, I want to see you.'\",\n",
    "            \" Bell’s invention revolutionized communication by allowing people to talk to each other over long distances.\"\n",
    "            ],\n",
    "            [\n",
    "            \"Thomas Edison is widely known for inventing the first commercially practical incandescent light bulb.\",\n",
    "            \" Although he did not invent the concept of the light bulb itself, Edison developed a version that was safe, affordable, and long-lasting.\",\n",
    "            \" His patent for the electric light bulb was filed in 1879, three years after Bell’s telephone patent.\",\n",
    "            \" Edison's innovation led to widespread use of electric lighting and helped usher in the modern electrical age.\"\n",
    "            ],\n",
    "            [\n",
    "            \"Before Edison, several inventors worked on early versions of the light bulb.\",\n",
    "            \" Sir Humphry Davy created the first electric arc lamp in the early 1800s, and later inventors like Joseph Swan in Britain improved upon the design.\",\n",
    "            \" However, these early bulbs were inefficient or burned out quickly, and it was Edison who perfected the design for everyday use.\"\n",
    "            ],\n",
    "            [\n",
    "            \"The telephone was invented before the practical light bulb.\",\n",
    "            \" Bell’s patent for the telephone was issued in 1876, while Edison’s patent for the light bulb was filed in 1879.\",\n",
    "            \" Thus, the telephone came first.\"\n",
    "            ],\n",
    "            [\n",
    "            \"Both the telephone and the light bulb are considered groundbreaking inventions of the late 19th century.\",\n",
    "            \" The telephone transformed communication, while the light bulb transformed how people lived and worked at night.\",\n",
    "            \" Together, they symbolize the rapid technological progress of that era.\"\n",
    "            ],\n",
    "            [\n",
    "            \"Edison and Bell were contemporaries and pioneers of the Second Industrial Revolution.\",\n",
    "            \" Their inventions marked major milestones in human history, driving the growth of telecommunications and electrical infrastructure.\"\n",
    "            ],\n",
    "            [\n",
    "            \"In summary, the telephone was invented in 1876 and the light bulb in 1879.\",\n",
    "            \" Therefore, the invention of the telephone came first.\"\n",
    "            ]\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e10adb",
   "metadata": {},
   "source": [
    "Next, we apply chat template and obtain the input range using the helper function defined above.<br />\n",
    "Specifically, as core reranker relies on the aggregated attentions from the user query to each passage, it needs a reference attention baseline for each passage. The authors swap the user query with `'N/A'` and treat the resulting aggregated attention as the normalizing factor for each passage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6fc0f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = case[\"query\"]\n",
    "documents = case[\"documents\"]\n",
    "        \n",
    "# Apply chat template and get ranges\n",
    "query_text, query_spec = apply_chat_template_and_get_ranges(llm.tokenizer, model, query, documents)\n",
    "na_text, na_spec = apply_chat_template_and_get_ranges(llm.tokenizer, model, 'N/A', documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4158c4d5",
   "metadata": {},
   "source": [
    "Finally, we perform the model inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "663cba6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged run ID.\n",
      "Created hook flag.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 618.08it/s]\n",
      "Processed prompts: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.61s/it, est. speed input: 363.81 toks/s, output: 0.62 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hooks deactivated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 568.64it/s]\n",
      "Processed prompts: 100%|████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 64.62it/s, est. speed input: 38989.81 toks/s, output: 66.60 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged run ID.\n",
      "Created hook flag.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 693.73it/s]\n",
      "Processed prompts: 100%|████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.66it/s, est. speed input: 16795.39 toks/s, output: 29.29 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hooks deactivated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 584.90it/s]\n",
      "Processed prompts: 100%|████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 71.72it/s, est. speed input: 42564.04 toks/s, output: 74.21 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=3, prompt=\"[INST] Here are some paragraphs:\\n\\n[document 1] Alexander Graham Bell is credited with inventing the first practical telephone.  He was awarded the U.S. patent for the invention of the telephone on March 7, 1876.  The first successful demonstration of the telephone took place shortly thereafter, when Bell famously called his assistant, saying, 'Mr. Watson, come here, I want to see you.'  Bell’s invention revolutionized communication by allowing people to talk to each other over long distances.\\n\\n[document 2] Thomas Edison is widely known for inventing the first commercially practical incandescent light bulb.  Although he did not invent the concept of the light bulb itself, Edison developed a version that was safe, affordable, and long-lasting.  His patent for the electric light bulb was filed in 1879, three years after Bell’s telephone patent.  Edison's innovation led to widespread use of electric lighting and helped usher in the modern electrical age.\\n\\n[document 3] Before Edison, several inventors worked on early versions of the light bulb.  Sir Humphry Davy created the first electric arc lamp in the early 1800s, and later inventors like Joseph Swan in Britain improved upon the design.  However, these early bulbs were inefficient or burned out quickly, and it was Edison who perfected the design for everyday use.\\n\\n[document 4] The telephone was invented before the practical light bulb.  Bell’s patent for the telephone was issued in 1876, while Edison’s patent for the light bulb was filed in 1879.  Thus, the telephone came first.\\n\\n[document 5] Both the telephone and the light bulb are considered groundbreaking inventions of the late 19th century.  The telephone transformed communication, while the light bulb transformed how people lived and worked at night.  Together, they symbolize the rapid technological progress of that era.\\n\\n[document 6] Edison and Bell were contemporaries and pioneers of the Second Industrial Revolution.  Their inventions marked major milestones in human history, driving the growth of telecommunications and electrical infrastructure.\\n\\n[document 7] In summary, the telephone was invented in 1876 and the light bulb in 1879.  Therefore, the invention of the telephone came first.\\n\\nPlease find information that are relevant to the following query in the paragraphs above.\\n\\nQuery: N/A[/INST]\", prompt_token_ids=[1, 1501, 17057, 29561, 4771, 1228, 1509, 19206, 29481, 29515, 781, 781, 29560, 7968, 29473, 29508, 29561, 11823, 18731, 12163, 1117, 4761, 2113, 1163, 9950, 1056, 1040, 1675, 11886, 16197, 29491, 29473, 1418, 1171, 14827, 1040, 1268, 29491, 29503, 29491, 24213, 1122, 1040, 22591, 1070, 1040, 16197, 1124, 4885, 29473, 29555, 29493, 29473, 29508, 29551, 29555, 29552, 29491, 29473, 1183, 1675, 6821, 7463, 1120, 1070, 1040, 16197, 2824, 2401, 17202, 1504, 8560, 29493, 1507, 12163, 2050, 5787, 2755, 1284, 14660, 29493, 4445, 29493, 1232, 19358, 29491, 23371, 29493, 2335, 2004, 29493, 1083, 1715, 1066, 1800, 1136, 2583, 29473, 12163, 29577, 29481, 22591, 11797, 2100, 9288, 1254, 10604, 1673, 1066, 2753, 1066, 2198, 1567, 1522, 1811, 24992, 29491, 781, 781, 29560, 7968, 29473, 29518, 29561, 6851, 1181, 2789, 1034, 1117, 13343, 3419, 1122, 9950, 1056, 1040, 1675, 6770, 29485, 2697, 11886, 3026, 1159, 1042, 2244, 2829, 6546, 29494, 29491, 29473, 6568, 1168, 1631, 1227, 9950, 1040, 6703, 1070, 1040, 2829, 6546, 29494, 4605, 29493, 1181, 2789, 1034, 6970, 1032, 3519, 1137, 1171, 5791, 29493, 17430, 29493, 1072, 1811, 29501, 4849, 1056, 29491, 29473, 3122, 24213, 1122, 1040, 8546, 2829, 6546, 29494, 1171, 17459, 1065, 29473, 29508, 29551, 29555, 29542, 29493, 2480, 2035, 1792, 12163, 29577, 29481, 16197, 24213, 29491, 29473, 1181, 2789, 1034, 29510, 29481, 17631, 4931, 1066, 24491, 1706, 1070, 8546, 16680, 1072, 7249, 1360, 1431, 1065, 1040, 5406, 18596, 4363, 29491, 781, 781, 29560, 7968, 29473, 29538, 29561, 8105, 1181, 2789, 1034, 29493, 3624, 9950, 1502, 4966, 1124, 3703, 11799, 1070, 1040, 2829, 6546, 29494, 29491, 29473, 9119, 1150, 14938, 1411, 4244, 29492, 4627, 1040, 1675, 8546, 14373, 21925, 1065, 1040, 3703, 29473, 29508, 29551, 29502, 29502, 29481, 29493, 1072, 2830, 9950, 1502, 1505, 8986, 4672, 1044, 1065, 10942, 12725, 4482, 1040, 3389, 29491, 29473, 3761, 29493, 1935, 3703, 6546, 2584, 1422, 1065, 29395, 1210, 15286, 1343, 5145, 29493, 1072, 1146, 1171, 1181, 2789, 1034, 1461, 4427, 1054, 1040, 3389, 1122, 15816, 1706, 29491, 781, 781, 29560, 7968, 29473, 29549, 29561, 1183, 16197, 1171, 26740, 1927, 1040, 11886, 2829, 6546, 29494, 29491, 29473, 12163, 29577, 29481, 24213, 1122, 1040, 16197, 1171, 12561, 1065, 29473, 29508, 29551, 29555, 29552, 29493, 2080, 1181, 2789, 1034, 29577, 29481, 24213, 1122, 1040, 2829, 6546, 29494, 1171, 17459, 1065, 29473, 29508, 29551, 29555, 29542, 29491, 29473, 7878, 29493, 1040, 16197, 2756, 1675, 29491, 781, 781, 29560, 7968, 29473, 29550, 29561, 8597, 1040, 16197, 1072, 1040, 2829, 6546, 29494, 1228, 5293, 4182, 26692, 9950, 1362, 1070, 1040, 4677, 29473, 29508, 29542, 1130, 6213, 29491, 29473, 1183, 16197, 19020, 9288, 29493, 2080, 1040, 2829, 6546, 29494, 19020, 1678, 1673, 7030, 1072, 4966, 1206, 2893, 29491, 29473, 22351, 29493, 1358, 6370, 1421, 1040, 9189, 27283, 5865, 1070, 1137, 4972, 29491, 781, 781, 29560, 7968, 29473, 29552, 29561, 1181, 2789, 1034, 1072, 12163, 1422, 24309, 5606, 1072, 22003, 1172, 1070, 1040, 7820, 23070, 14618, 29491, 29473, 7491, 9950, 1362, 11495, 3782, 2888, 1142, 3170, 1065, 3698, 4108, 29493, 8578, 1040, 6825, 1070, 7352, 16744, 11167, 1072, 18596, 15229, 29491, 781, 781, 29560, 7968, 29473, 29555, 29561, 1328, 14828, 29493, 1040, 16197, 1171, 26740, 1065, 29473, 29508, 29551, 29555, 29552, 1072, 1040, 2829, 6546, 29494, 1065, 29473, 29508, 29551, 29555, 29542, 29491, 29473, 9237, 29493, 1040, 22591, 1070, 1040, 16197, 2756, 1675, 29491, 781, 781, 12837, 2068, 2639, 1137, 1228, 9366, 1066, 1040, 3064, 6477, 1065, 1040, 19206, 29481, 3515, 29491, 781, 781, 4062, 29515, 1186, 29516, 29509, 29560, 29516, 17057, 29561], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='\\n', token_ids=[781], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=560, multi_modal_placeholders={})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.generate(query_text, temperature=0.1, max_tokens=1)\n",
    "llm.generate(na_text, cleanup=False, temperature=0.1, max_tokens=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5981dd0a",
   "metadata": {},
   "source": [
    "During the model inference in the previous step, vLLM-Hook has automatically saved selected queries and keys. Now, we can directly call the analyzer to get the passage relevance score and the final ranking of passages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c281467f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stats = llm.analyze(analyzer_spec={'query_spec': query_spec, 'na_spec': na_spec})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d76736b",
   "metadata": {},
   "source": [
    "Finally we can print out the results as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b353ed22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted document IDs and scores by CoRe-Reranking: [[6, 3, 1, 0, 4, 2, 5]]: [[4.04296875, 3.427734375, 2.419921875, 1.6767578125, 1.62890625, 1.01953125, 0.79736328125]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sorted document IDs and scores by CoRe-Reranking: {stats['ranking']}: {stats['scores']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm_hook_env",
   "language": "python",
   "name": "vllm_hook_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
